{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc738cdc-29fc-46c2-bd71-9c2783982b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a660af172807\n"
     ]
    }
   ],
   "source": [
    "!hostname #main container name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4e0e6-934e-4cfe-84da-fb297d15cc4c",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df964c4-43dc-4b1e-a293-276925867b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-31 02:40:08--  https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv\n",
      "Resolving pages.cs.wisc.edu (pages.cs.wisc.edu)... 128.105.7.9\n",
      "Connecting to pages.cs.wisc.edu (pages.cs.wisc.edu)|128.105.7.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 174944099 (167M) [text/csv]\n",
      "Saving to: ‘hdma-wi-2021.csv’\n",
      "\n",
      "hdma-wi-2021.csv    100%[===================>] 166.84M  8.26MB/s    in 25s     \n",
      "\n",
      "2023-03-31 02:40:33 (6.73 MB/s) - ‘hdma-wi-2021.csv’ saved [174944099/174944099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d86fc4e-f96f-48ad-bf14-a7ccbc468019",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.block.size=1M -D dfs.replication=1 -cp hdma-wi-2021.csv hdfs://main:9000/single.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e04905-f68f-4cc2-8db2-c35a44e2f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.block.size=1M -D dfs.replication=2 -cp hdma-wi-2021.csv hdfs://main:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4ae446-35f3-488f-a519-a4875adc6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.8 M  333.7 M  hdfs://main:9000/double.csv\n",
      "166.8 M  166.8 M  hdfs://main:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h hdfs://main:9000/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fdceb-ce89-4c39-bb2a-2f7696e22066",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0907e8-1929-4206-b1d4-1f3512ba1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "350947f8-a0f5-4af1-9849-a4bee359821a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'09d1c50c76cf:9864/webhdfs/v1/single.csv': 94,\n",
       " '8f93d9669d45:9864/webhdfs/v1/single.csv': 73}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_blocks={}\n",
    "x=0\n",
    "while x<174692761: # up to 167 MBs in bytes\n",
    "    req = requests.get(f'http://main:9870/webhdfs/v1/single.csv?op=OPEN&offset={x}', allow_redirects=False)\n",
    "    if req.headers[\"Location\"][7:46] in d_blocks:\n",
    "        d_blocks[req.headers[\"Location\"][7:46]]=d_blocks[req.headers[\"Location\"][7:46]] + 1\n",
    "    else: \n",
    "        d_blocks[req.headers[\"Location\"][7:46]]=1\n",
    "    x+=1048576\n",
    "\n",
    "d_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7aa09-005e-4da3-bcfb-f2b43ce4bed1",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfe5c28-8118-4c7f-a96e-d07f2129b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "846970c5-b06b-4f36-b06d-fa924e6ebe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdfsFile(io.RawIOBase):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.offset = 0 #bytes I read\n",
    "        \n",
    "        res = requests.get(f'http://main:9870/webhdfs/v1/{self.path}?op=GETFILESTATUS', allow_redirects=False)\n",
    "        self.length = res.json()['FileStatus']['length']\n",
    "\n",
    "        \n",
    "    def readable(self):\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def readinto(self, b):\n",
    "        if self.offset >= self.length:\n",
    "            return 0\n",
    "        \n",
    "        bytes_todo = min(len(b), self.length - self.offset)\n",
    "        \n",
    "        url = f'http://main:9870/webhdfs/v1/{self.path}'\n",
    "        ress = requests.get(url=url, params={\"op\": \"OPEN\", \"offset\": self.offset, \"length\": bytes_todo})\n",
    "        \n",
    "        if ress.status_code != 200:\n",
    "            b[0:1] = b\"\\n\"\n",
    "            self.offset = (self.offset // 1024 + 1) * 1024  #start of next block\n",
    "            return 1\n",
    "        \n",
    "        b[0:bytes_todo] = bytearray(ress.content)\n",
    "        self.offset += bytes_todo\n",
    "\n",
    "        return bytes_todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7842053-3eff-48cd-8bb6-4c098bb46d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv \n",
      "Single Family: 444874 \n",
      "Multi Family: 2493 \n",
      "Seconds: 42.1686737537384\n"
     ]
    }
   ],
   "source": [
    "#size 1\n",
    "Single_fam=0\n",
    "Multi_fam=0\n",
    "t0 = time.time()\n",
    "\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"), buffer_size=100000):\n",
    "    line = str(line, \"utf-8\")\n",
    "    #print(line)\n",
    "    if \"Single Family\" in line:\n",
    "        Single_fam+=1\n",
    "    if \"Multifamily\" in line:\n",
    "        Multi_fam+=1\n",
    "    \n",
    "end=time.time()-t0\n",
    "print(\"Counts from single.csv\", \"\\nSingle Family:\", Single_fam, \"\\nMulti Family:\", Multi_fam, \"\\nSeconds:\", end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d7eb3d5-0860-46da-9a15-f81f2992d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv \n",
      "Single Family: 444874 \n",
      "Multi Family: 2493 \n",
      "Seconds: 6.35726261138916\n"
     ]
    }
   ],
   "source": [
    "#size 2\n",
    "Single_fam=0\n",
    "Multi_fam=0\n",
    "t0 = time.time()\n",
    "\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"), buffer_size=1024**2):\n",
    "    line = str(line, \"utf-8\")\n",
    "    #print(line)\n",
    "    if \"Single Family\" in line:\n",
    "        Single_fam+=1\n",
    "    if \"Multifamily\" in line:\n",
    "        Multi_fam+=1\n",
    "    \n",
    "end=time.time()-t0\n",
    "\n",
    "print(\"Counts from single.csv\", \"\\nSingle Family:\", Single_fam, \"\\nMulti Family:\", Multi_fam, \"\\nSeconds:\", end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2db8a-4558-46cf-8694-8e1dff01d2f2",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a5d90-e75f-4c19-9ced-0ce11c1e5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfsadmin -fs hdfs://main:9000/ -report  #run after docker kill, I did worker 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dea0e3-2128-486a-8e4c-14ef83efa5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_fam=0\n",
    "Multi_fam=0\n",
    "\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"), buffer_size=1024**2):\n",
    "    line = str(line, \"utf-8\")\n",
    "    #print(line)\n",
    "    if \"Single Family\" in line:\n",
    "        Single_fam+=1\n",
    "    if \"Multifamily\" in line:\n",
    "        Multi_fam+=1\n",
    "\n",
    "print(\"Counts from single.csv\", \"\\nSingle Family:\", Single_fam, \"\\nMulti Family:\", Multi_fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822b168-beba-4a78-98ee-c313813d989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_fam=0\n",
    "Multi_fam=0\n",
    "\n",
    "for line in io.BufferedReader(hdfsFile(\"double.csv\"), buffer_size=1024**2):\n",
    "    line = str(line, \"utf-8\")\n",
    "    #print(line)\n",
    "    if \"Single Family\" in line:\n",
    "        Single_fam+=1\n",
    "    if \"Multifamily\" in line:\n",
    "        Multi_fam+=1\n",
    "    \n",
    "\n",
    "print(\"Counts from double.csv\", \"\\nSingle Family:\", Single_fam, \"\\nMulti Family:\", Multi_fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97b676-2a20-4bb7-941d-c456ce375014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
